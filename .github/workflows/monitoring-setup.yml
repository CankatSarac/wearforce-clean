name: Monitoring and Observability Setup

on:
  push:
    branches: [main]
    paths:
      - 'k8s/monitoring/**'
      - 'infrastructure/terraform/modules/monitoring/**'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to setup monitoring'
        required: true
        default: 'production'
        type: choice
        options:
          - production
          - staging
          - development

env:
  HELM_VERSION: 'v3.14.0'
  KUBECTL_VERSION: 'v1.29.0'

jobs:
  # Deploy Prometheus Stack
  prometheus-stack:
    name: Deploy Prometheus Stack
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Kubectl
      uses: azure/setup-kubectl@v4
      with:
        version: ${{ env.KUBECTL_VERSION }}

    - name: Setup Helm
      uses: azure/setup-helm@v4
      with:
        version: ${{ env.HELM_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2

    - name: Update kubeconfig
      run: |
        ENV="${{ github.event.inputs.environment || 'production' }}"
        aws eks update-kubeconfig --region us-west-2 --name wearforce-${ENV}

    - name: Add Helm repositories
      run: |
        helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
        helm repo add grafana https://grafana.github.io/helm-charts
        helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
        helm repo add elastic https://helm.elastic.co
        helm repo update

    - name: Create monitoring namespace
      run: |
        kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -

    - name: Deploy Prometheus Operator
      run: |
        helm upgrade --install prometheus-operator prometheus-community/kube-prometheus-stack \
          --namespace monitoring \
          --values k8s/monitoring/prometheus/values-production.yaml \
          --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=50Gi \
          --set prometheus.prometheusSpec.retention=30d \
          --set prometheus.prometheusSpec.retentionSize=45GiB \
          --set alertmanager.alertmanagerSpec.storage.volumeClaimTemplate.spec.resources.requests.storage=10Gi \
          --wait --timeout=10m

    - name: Configure Prometheus rules
      run: |
        kubectl apply -f k8s/monitoring/prometheus/rules/ -n monitoring
        
    - name: Setup Prometheus ServiceMonitors
      run: |
        kubectl apply -f k8s/monitoring/prometheus/servicemonitors/ -n monitoring

  # Deploy Grafana with Custom Dashboards
  grafana-setup:
    name: Setup Grafana Dashboards
    runs-on: ubuntu-latest
    needs: [prometheus-stack]
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Kubectl
      uses: azure/setup-kubectl@v4
      with:
        version: ${{ env.KUBECTL_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2

    - name: Update kubeconfig
      run: |
        ENV="${{ github.event.inputs.environment || 'production' }}"
        aws eks update-kubeconfig --region us-west-2 --name wearforce-${ENV}

    - name: Import Grafana dashboards
      run: |
        # Wait for Grafana to be ready
        kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=grafana -n monitoring --timeout=300s
        
        # Get Grafana admin password
        GRAFANA_PASSWORD=$(kubectl get secret prometheus-operator-grafana -n monitoring -o jsonpath="{.data.admin-password}" | base64 --decode)
        
        # Port forward to Grafana (in background)
        kubectl port-forward svc/prometheus-operator-grafana 3000:80 -n monitoring &
        GRAFANA_PID=$!
        
        # Wait for port forward
        sleep 10
        
        # Import custom dashboards
        for dashboard in k8s/monitoring/grafana/dashboards/*.json; do
          if [ -f "$dashboard" ]; then
            echo "Importing dashboard: $dashboard"
            curl -X POST \
              -H "Content-Type: application/json" \
              -u admin:${GRAFANA_PASSWORD} \
              -d @"$dashboard" \
              http://localhost:3000/api/dashboards/db
          fi
        done
        
        # Kill port forward
        kill $GRAFANA_PID || true

    - name: Configure Grafana data sources
      run: |
        kubectl apply -f k8s/monitoring/grafana/datasources/ -n monitoring

  # Deploy Jaeger for Distributed Tracing
  jaeger-tracing:
    name: Deploy Jaeger Tracing
    runs-on: ubuntu-latest
    needs: [prometheus-stack]
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Kubectl
      uses: azure/setup-kubectl@v4
      with:
        version: ${{ env.KUBECTL_VERSION }}

    - name: Setup Helm
      uses: azure/setup-helm@v4
      with:
        version: ${{ env.HELM_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2

    - name: Update kubeconfig
      run: |
        ENV="${{ github.event.inputs.environment || 'production' }}"
        aws eks update-kubeconfig --region us-west-2 --name wearforce-${ENV}

    - name: Deploy Jaeger Operator
      run: |
        kubectl create namespace jaeger --dry-run=client -o yaml | kubectl apply -f -
        kubectl apply -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.51.0/jaeger-operator.yaml -n jaeger

    - name: Deploy Jaeger instance
      run: |
        kubectl apply -f k8s/monitoring/jaeger/jaeger-production.yaml -n jaeger
        
        # Wait for Jaeger to be ready
        kubectl wait --for=condition=ready pod -l app=jaeger -n jaeger --timeout=300s

  # Deploy ELK Stack for Centralized Logging
  elasticsearch-logging:
    name: Deploy Elasticsearch & Kibana
    runs-on: ubuntu-latest
    needs: [prometheus-stack]
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Kubectl
      uses: azure/setup-kubectl@v4
      with:
        version: ${{ env.KUBECTL_VERSION }}

    - name: Setup Helm
      uses: azure/setup-helm@v4
      with:
        version: ${{ env.HELM_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2

    - name: Update kubeconfig
      run: |
        ENV="${{ github.event.inputs.environment || 'production' }}"
        aws eks update-kubeconfig --region us-west-2 --name wearforce-${ENV}

    - name: Create logging namespace
      run: |
        kubectl create namespace logging --dry-run=client -o yaml | kubectl apply -f -

    - name: Deploy Elasticsearch
      run: |
        helm upgrade --install elasticsearch elastic/elasticsearch \
          --namespace logging \
          --set replicas=3 \
          --set minimumMasterNodes=2 \
          --set esConfig."elasticsearch\.yml"="cluster.name: \"wearforce-logs\"\nnetwork.host: 0.0.0.0" \
          --set resources.requests.cpu="1000m" \
          --set resources.requests.memory="2Gi" \
          --set resources.limits.cpu="2000m" \
          --set resources.limits.memory="4Gi" \
          --set volumeClaimTemplate.resources.requests.storage="30Gi" \
          --wait --timeout=10m

    - name: Deploy Kibana
      run: |
        helm upgrade --install kibana elastic/kibana \
          --namespace logging \
          --set elasticsearchHosts="http://elasticsearch-master:9200" \
          --set resources.requests.cpu="500m" \
          --set resources.requests.memory="1Gi" \
          --set resources.limits.cpu="1000m" \
          --set resources.limits.memory="2Gi" \
          --wait --timeout=10m

    - name: Deploy Filebeat
      run: |
        helm upgrade --install filebeat elastic/filebeat \
          --namespace logging \
          --set daemonset.enabled=true \
          --set deployment.enabled=false \
          --set filebeatConfig."filebeat.yml"="filebeat.autodiscover:\n  providers:\n    - type: kubernetes\n      node: \${NODE_NAME}\n      hints.enabled: true\n      hints.default_config:\n        type: container\n        paths:\n          - /var/log/containers/*\${data.kubernetes.container.id}.log\nprocessors:\n  - add_kubernetes_metadata:\n      host: \${NODE_NAME}\n      matchers:\n        - logs_path:\n            logs_path: \"/var/log/containers/\"\noutput.elasticsearch:\n  host: '\${NODE_NAME}'\n  hosts: ['elasticsearch-master:9200']" \
          --wait --timeout=5m

  # Deploy Fluentd for Log Processing
  fluentd-logging:
    name: Deploy Fluentd
    runs-on: ubuntu-latest
    needs: [elasticsearch-logging]
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Kubectl
      uses: azure/setup-kubectl@v4
      with:
        version: ${{ env.KUBECTL_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2

    - name: Update kubeconfig
      run: |
        ENV="${{ github.event.inputs.environment || 'production' }}"
        aws eks update-kubeconfig --region us-west-2 --name wearforce-${ENV}

    - name: Deploy Fluentd
      run: |
        kubectl apply -f k8s/monitoring/fluentd/ -n logging
        
        # Wait for Fluentd to be ready
        kubectl wait --for=condition=ready pod -l app=fluentd -n logging --timeout=300s

  # Setup Alertmanager and notification channels
  alerting-setup:
    name: Setup Alerting
    runs-on: ubuntu-latest
    needs: [prometheus-stack]
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Kubectl
      uses: azure/setup-kubectl@v4
      with:
        version: ${{ env.KUBECTL_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2

    - name: Update kubeconfig
      run: |
        ENV="${{ github.event.inputs.environment || 'production' }}"
        aws eks update-kubeconfig --region us-west-2 --name wearforce-${ENV}

    - name: Configure Alertmanager
      run: |
        # Create alertmanager configuration secret
        kubectl create secret generic alertmanager-main \
          --from-file=k8s/monitoring/alertmanager/alertmanager.yml \
          -n monitoring \
          --dry-run=client -o yaml | kubectl apply -f -
        
        # Restart alertmanager to pick up config
        kubectl rollout restart statefulset/alertmanager-prometheus-operator-kube-p-alertmanager -n monitoring

    - name: Setup PagerDuty integration
      if: env.PAGERDUTY_SERVICE_KEY != ''
      run: |
        # Configure PagerDuty webhook
        kubectl create secret generic pagerduty-secret \
          --from-literal=service-key="${{ secrets.PAGERDUTY_SERVICE_KEY }}" \
          -n monitoring \
          --dry-run=client -o yaml | kubectl apply -f -

    - name: Setup Slack integration
      if: env.SLACK_WEBHOOK_URL != ''
      run: |
        # Configure Slack webhook
        kubectl create secret generic slack-webhook \
          --from-literal=webhook-url="${{ secrets.SLACK_WEBHOOK_URL }}" \
          -n monitoring \
          --dry-run=client -o yaml | kubectl apply -f -

    - name: Test alerting
      run: |
        # Create a test alert to verify configuration
        kubectl apply -f - <<EOF
        apiVersion: monitoring.coreos.com/v1
        kind: PrometheusRule
        metadata:
          name: test-alert
          namespace: monitoring
        spec:
          groups:
          - name: test
            rules:
            - alert: TestAlert
              expr: vector(1)
              for: 0m
              labels:
                severity: info
              annotations:
                summary: "Test alert to verify monitoring setup"
        EOF
        
        # Wait a moment then clean up
        sleep 60
        kubectl delete prometheusrule test-alert -n monitoring || true

  # Setup custom metrics and SLIs
  custom-metrics:
    name: Setup Custom Metrics
    runs-on: ubuntu-latest
    needs: [prometheus-stack]
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Kubectl
      uses: azure/setup-kubectl@v4
      with:
        version: ${{ env.KUBECTL_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2

    - name: Update kubeconfig
      run: |
        ENV="${{ github.event.inputs.environment || 'production' }}"
        aws eks update-kubeconfig --region us-west-2 --name wearforce-${ENV}

    - name: Deploy custom ServiceMonitors
      run: |
        # Deploy ServiceMonitors for WearForce services
        kubectl apply -f k8s/monitoring/servicemonitors/ -n monitoring

    - name: Deploy SLI/SLO rules
      run: |
        # Deploy custom Prometheus rules for SLIs/SLOs
        kubectl apply -f k8s/monitoring/sli-slo-rules/ -n monitoring

    - name: Setup recording rules
      run: |
        # Deploy recording rules for performance metrics
        kubectl apply -f k8s/monitoring/recording-rules/ -n monitoring

  # Performance testing and monitoring validation
  monitoring-validation:
    name: Validate Monitoring Setup
    runs-on: ubuntu-latest
    needs: [prometheus-stack, grafana-setup, jaeger-tracing, elasticsearch-logging, alerting-setup]
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Kubectl
      uses: azure/setup-kubectl@v4
      with:
        version: ${{ env.KUBECTL_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2

    - name: Update kubeconfig
      run: |
        ENV="${{ github.event.inputs.environment || 'production' }}"
        aws eks update-kubeconfig --region us-west-2 --name wearforce-${ENV}

    - name: Validate Prometheus targets
      run: |
        # Port forward to Prometheus
        kubectl port-forward svc/prometheus-operator-kube-p-prometheus 9090:9090 -n monitoring &
        PROM_PID=$!
        sleep 10
        
        # Check target health
        TARGETS=$(curl -s http://localhost:9090/api/v1/targets | jq -r '.data.activeTargets[] | select(.health != "up") | .scrapeUrl')
        if [ -n "$TARGETS" ]; then
          echo "⚠️  Unhealthy targets found:"
          echo "$TARGETS"
        else
          echo "✅ All Prometheus targets are healthy"
        fi
        
        kill $PROM_PID || true

    - name: Validate Grafana dashboards
      run: |
        # Port forward to Grafana
        kubectl port-forward svc/prometheus-operator-grafana 3000:80 -n monitoring &
        GRAFANA_PID=$!
        sleep 10
        
        # Check if Grafana is accessible
        if curl -f http://localhost:3000/api/health; then
          echo "✅ Grafana is accessible"
        else
          echo "❌ Grafana health check failed"
        fi
        
        kill $GRAFANA_PID || true

    - name: Validate Elasticsearch cluster
      run: |
        # Port forward to Elasticsearch
        kubectl port-forward svc/elasticsearch-master 9200:9200 -n logging &
        ES_PID=$!
        sleep 10
        
        # Check cluster health
        HEALTH=$(curl -s http://localhost:9200/_cluster/health | jq -r '.status')
        if [ "$HEALTH" = "green" ] || [ "$HEALTH" = "yellow" ]; then
          echo "✅ Elasticsearch cluster is healthy: $HEALTH"
        else
          echo "❌ Elasticsearch cluster is unhealthy: $HEALTH"
        fi
        
        kill $ES_PID || true

    - name: Validate Jaeger tracing
      run: |
        # Check if Jaeger is running
        if kubectl get pods -n jaeger | grep jaeger | grep Running; then
          echo "✅ Jaeger is running"
        else
          echo "❌ Jaeger is not running properly"
        fi

    - name: Generate monitoring summary
      run: |
        echo "# Monitoring Setup Summary" > monitoring-summary.md
        echo "## Deployed Components" >> monitoring-summary.md
        echo "- ✅ Prometheus & Alertmanager" >> monitoring-summary.md
        echo "- ✅ Grafana with Custom Dashboards" >> monitoring-summary.md
        echo "- ✅ Jaeger Distributed Tracing" >> monitoring-summary.md
        echo "- ✅ Elasticsearch & Kibana Logging" >> monitoring-summary.md
        echo "- ✅ Fluentd Log Processing" >> monitoring-summary.md
        echo "" >> monitoring-summary.md
        echo "## Access URLs" >> monitoring-summary.md
        echo "- **Prometheus**: https://prometheus.wearforce.io" >> monitoring-summary.md
        echo "- **Grafana**: https://grafana.wearforce.io" >> monitoring-summary.md
        echo "- **Kibana**: https://kibana.wearforce.io" >> monitoring-summary.md
        echo "- **Jaeger**: https://jaeger.wearforce.io" >> monitoring-summary.md
        echo "" >> monitoring-summary.md
        echo "## Resource Usage" >> monitoring-summary.md
        kubectl top nodes >> monitoring-summary.md || echo "Node metrics not available" >> monitoring-summary.md
        echo "" >> monitoring-summary.md
        kubectl get pods -n monitoring -o wide >> monitoring-summary.md
        kubectl get pods -n logging -o wide >> monitoring-summary.md
        kubectl get pods -n jaeger -o wide >> monitoring-summary.md

    - name: Upload monitoring summary
      uses: actions/upload-artifact@v4
      with:
        name: monitoring-setup-summary
        path: monitoring-summary.md